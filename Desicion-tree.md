# 决策树

决策树算法即可以作为**分类算法**, 也可以作为**回归算法**, 同时也特别适合**集成学习**. 这里面需要了解的是`ID3`, `C4.5`, `CART`三类算法. 并且掌握它的算法实现.

决策树的核心需要解决的方法就是, 在走到一个节点之后, 应该如何**选择合适的特征**作为分叉的标准.

## ID3

### 信息熵

信息熵可以说是ID3算法的核心. 熵代表着事物的不确定性, **不确定性越高, 则熵越大**. 信息熵的表达式如下所示.
$$
H(X) = -\sum_{x \in X}P(X = x)logP(X = x)
$$
其中$x$代表$X$的不同取值, 而$p_i$则代表$X$取值为$i$时的概率, $log$是以2为底或者是以e为底的对数. 

例如当$X$为2时, 此时只有两个概率取值均为1/2时, 信息熵的值才会最大, 即此时的不确定性最大.

==从表达式也可以看出信息熵是各事件发生的一个期望值.==

#### 联合信息熵

此外, 我们也可以推出多个变量的联合熵的表达式.
$$
H(X, Y) = -\sum_{x \in X}\sum_{y \in Y}P(X=x, Y=y)logP(X=x, Y=y)
$$
联合熵表达式还有其他的一些写法, 但我认为这样的写法最容易理解, 即遍历$X$和$Y$的取值, 求所有功能的信息熵再求和.

#### 条件熵

条件熵的物理意义是在得知某一确定信息的前提下, 另一条信息的不确定性. 我们可以得到条件熵的公式及其推导过程如下所示.
$$
\begin{aligned}
H(Y|X) &= \sum_{x \in X}P(X=x)H(Y|X=x) \\
       &= -\sum_{x \in X}P(X=x)\sum_{y \in Y}P(y|X=x)logP(y|X=x) \\
       &= -\sum_{x \in X}\sum_{y \in Y}P(X=x)P(y|X=x)logP(y|X=x) \\
       &= -\sum_{x \in X}\sum_{y \in Y}P(X=x, Y=y)logP(y|X=x) \\
       &= H(X, Y) - H(X)
\end{aligned}
$$
==需要注意, 这个条件熵和条件概率是不同的.==条件概率是指在变量$X$取某一个值时, 另一个变量发生的概率是多少. 而在推导条件熵的过程中, 会用到类似的概念, 我把这个值称为**类条件熵**, 其具体的公式如下所示.
$$
H(Y|X=x) = -\sum_{y \in Y}P(y|X=x)logP(y|X=x)
$$
从而我们可以发现, 条件熵其实是类条件熵的一个**期望**, 即遍历所有的$X$的取值获得的期望. $\color{#0000FF}{不要把条件熵的概念和类条件熵混淆了!}$

### 信息增益

$$
I(X, Y) = H(X) - H(X|Y)
$$

这个物理意义是比较好理解的, 即体现了在知道信息的Y的条件下, X的不确定性下降的程度.

### 算法流程

ID3中使用`信息增益`作为判断依据, 来选择特征构建决策树. 具体的算法过程如下.

> 1. 初始化信息增益阈值
> 2. 判断样本是否为同一类输出, 如果是则返回单节点树, 并标记类别
> 3. 判断特征是否为空, 如果是则返回单节点树, 标记类别为类别样本数最多的类别
> 4. 计算各个特征对输出的信息增益, 选择信息增益最大的特征
> 5. 如果该特征的信息增益小于阈值, 则返回单节点树, 标记类别为类别样本数量最多的类别
> 6. 否则, 按照特征的取值分成不同的分支, 返回增加的节点数
> 7. 对于所有的子节点, 递归的调用2-6步, 从而得到子树返回.

### 不足

- 没有考虑连续特征, 如密度等, 限制了应用范围
- 由于采用了信息增益, 导致被发现特征取值比较多的情况下其信息增益大. 即**更倾向于取值多的特征**
- 没有考虑缺失值的情况
- 没有应对过拟合的措施

## C4.5

C4.5算法主要是针对于ID3中的不足提出了一些改进方法.

### 针对不适用连续特征问题

采用的方法是将连续特征进行离散化. 例如, 对于$m$个样本, 选择$m-1$个划分点, 对于每一个划分点计算信息增益, 从而选择最优的划分点将样本划分成两类, 从而实现连续特征的离散化.

### 针对更偏向于取值多的特征的问题

引入了信息增益比,  表达式如下所示.
$$
I_R(D, A) = \frac{I(D, A)}{H_A(D)}
$$
信息增益比是在原来的基础上除以$H_A(D)$得到的, 我们称它为信息熵, 即具体的表达式如下.
$$
H_A(D) = -\sum_{i=1}^{n}\frac{|D_i|}{|D|}log\frac{|D_i|}{|D|}
$$
其中$n$为特征$A$的类别数, $|D|$为样本个数, $|D_i|$为特征取第i个值得样本数. 这个值具有**取值种类越多, 则每个取值的样本量越小, 则整体值越大**的特点. 因此将其作为分母, 可以有效的修正偏向取值多的特征的问题.

> 对于后面两个问题,  在CART中都有更好的解法, 故不在此赘述了.

### 不足

- 虽然针对过拟合使用了剪枝, 但还可以进一步的优化
- C4.5生成的是多叉树的模型, 而在计算机中, 二叉树的模型往往效率更高一些
- **C4.5只能用于分类**
- 由于使用了熵模型, 并且对连续变量进行了大量的排序运算, 所以运算强度大, 耗时严重.

## CART

无论是在ID3还是在C4.5当中, 均是使用熵模型来计算不确定度, 这带来了极大的计算压力. 而CART为了避免这个问题, 使用了**基尼系数**来替代信息增益比.

### 基尼系数

基尼系数代表了信息的不纯度, 即==基尼系数越小, 则纯度越高, 特征越好.==

具体的, 假设有$K$个类别, 其中第$k$个类别的概率是$P_k$, 则基尼系数的表达式如下所示.
$$
Gini(P) = \sum_{k=1}^{K}P_k(1-P_k) = 1 - \sum_{k=1}^{K}P_k^2
$$
而在实际的使用过程中, 各个类别的概率我们是不知道的, 从而使用样本进行估计, 得到如下的表达式. 其中$|C_k|$为取值为$k$的样本量.
$$
Gini(D) = 1 - \sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$
而如果根据特征$A$将样本划分成两个部分, 则在特征$A$的条件下, 其基尼系数的表达式如下所示.
$$
Gini(D, A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
$$
CART树通过使用基尼系数, 从而大大提高了运算效率. 同时为了进一步的简化, **CART每次仅仅对某一个特征的值进行二分**, 因此CART树是一棵``二叉树``. 这样可以进一步的简化运算.

### 对于连续值和离散值的处理

对于连续值的处理和C4.5的方法是一样的, 都是通过排序, 再选择相邻两值的均值作为一个划分点, 从而选择最优的划分点. 只不过C4.5中使用的是信息增益比, 而CART中使用的是基尼系数.

而对于离散值, 由于CART是一棵二叉树, 因此它采用的是不断二分. 如对于特征$A$要建立决策树节点, 它有$A_1, A_2, A_3$三个类别. 如果是在C4.5中, 会直接分出三个分支, 即不同的类别为不同的分支. ==但是在CART中, 只会分成两个分支, 如{$A_1$}{$A_2, A_3$}等.==并从中选择基尼系数最小的分割方法进行分割, 从而仅生成两个分支. 这是两者最大的区别.

### 算法流程

这里仅仅介绍的建立的算法流程, CART中还会涉及到剪枝的算法.

>输入: 训练集D, 基尼系数的阈值, 样本个体阈值
>输出: 决策树T
>
>1. 对于当前节点的数据集D, 如果其数量小于样本个体阈值, 则返回决策子树, 递归停止
>2. 计算当前数据集D的基尼指数, 如果基尼指数小于阈值, 则返回决策子树, 递归停止
>3. 计算当前各个特征和特征值的基尼指数, 对于离散值和连续值采用不同的策略
>4. 选择基尼系数最小的特征A和它对应的特征值a, 根据这个最优特征和最优特征值, 将数据集进行二分
>5. 对左右子树继续递归调用1-4步, 指导递归结束, 返回生成的决策树T

### CART回归树

回归树是CART树的一大特点, 因为ID3和C4.5都只能用于进行分类, 但CART树既可以用作分类, 也可以用作回归. 而分类树和回归树的区别主要在于输出值. 如果输出是离散的, 那么就是分类树. 如果输出是连续的, 那么就是回归树.

CART回归树在算法上和分类树基本都是相同的, 这里只是提一下不同点.

- 连续值的处理方式不同

  在回归树中, 对于连续值的处理, 使用的是`方差`的度量方式. **也正是这种度量方式的选择, 才能让CART具有回归的功能**. 因为无论是熵模型还是基尼系数, 都只适用于分类.

  CART回归树的度量目标是, 对于任意划分特征$A$, 对应的任意划分点$s$两边分成的数据集分别是$D_1$和$D_2$. 求出使各自集合的均方差最小, 同时均方差之和最小所对应的最优特征和最优划分点. 表达式如下所示. 其中$c_1$为数据集$D_1$的均值, $c_2$是数据集$D_2$的均值.

$$
\underbrace{min}_{A, s}[\underbrace{min}_{c_1}\sum_{x_i \in D_1(A, s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum_{x_i \in D_2(A, s)}(y_i - c_2)^2]
$$

- 预测方式的不同

  对于分类问题, CART输出的是节点中的样本量最多的类别. 而对于回归问题, CART输出的是**节点中的均值或者中位数**.

### CART中的剪枝

CART分类树和CART回归树核心的不同为**度量**损失的不同, 因此在剪枝的算法基本是一样的.

剪枝的作用主要是提高决策树的`泛化`能力. 剪枝的方法主要有两种, 即*预剪枝*和*后剪枝*. CART树使用的是**后剪枝**, 即先生成决策树后再进行剪枝, 剪枝主要由两步完成.

1. 从原始决策树中生成各种剪枝效果的决策树

2. 使用`交叉验证`来检验剪枝后的预测能力, 选择泛化预测能力最好的树作为剪枝后的决策树

首先我们需要度量一个子树的损失, 这样我们才能判断出它剪枝前后损失的变化, 具体的表达式如下所示.
$$
C_\alpha(T_t) = C(T_t) + \alpha|T|
$$
其中$\alpha$为正则化参数, $C(T_t)$为训练数据的预测误差, $|T|$为子树`叶子节点`的节点数量. 

可以发现, 当$\alpha=0$时, 则不需要剪枝. 当$\alpha=\infin$时, 此时以根节点构成的单节点树为最优子树. 从而可以看出, $\alpha$越大, 则剪枝的程度越厉害. 

而对于某一个节点, 如果没有剪枝, 我们可以知道它的损失如下所示.
$$
C_\alpha(T_t) = C(T_t) + \alpha|T|
$$
当只有一个根节点时, 它的损失如下所示.
$$
C_\alpha(T) = C(T) + \alpha
$$
初时, 当$\alpha$很小的时候, 明显$C_\alpha(T_t) < C_\alpha(T)$. 因为正则的作用小, 明显分的越细损失越小. 但当$\alpha$逐渐增大的过程中, 正则作用逐渐增强, 最终会导致$C_\alpha(T_t) = C_\alpha(T)$. `此时剪枝前后的损失是一样的`, 那我们当然会更倾向于剪枝. 也就是说, 如果满足下式.
$$
\alpha = \frac{C(T) - C(T_t)}{|T_t| - 1}
$$
则可以将它的子节点全部剪掉, 变为一个叶节点$T$, 从而实现剪枝.

因此, 我们可以把所有节点的$\alpha$计算出来, 然后使用不同的$\alpha$把对应的部分进行剪枝, 从而构成一个树的集合. 对这些树进行`交叉验证`, 从中找到一棵最优子树作为我们的最终结果.

### 剪枝算法流程

前面提到我们可以计算所有的$\alpha$值, 并将对应的节点进行剪枝来获得树的集合. 但实际上, 我们希望从最小的$\alpha$开始剪枝, **并且是在剪枝之后的结果上继续剪枝**. 可以从两个方面来理解这个方式:

- 越小的$\alpha$表明修剪的部分越小, 从而我们可以自下而上的对树进行修剪.
- 先剪$\alpha$小的节点, 将不会影响$\alpha$大的节点. 因为此时后者剪枝后损失会增大. 这也说明为什么我们可以在前面剪枝的基础上继续剪枝了.

根据以上的理解, 从而可以得到我们的剪枝算法流程. **李航**老师的原版书上有一定的错误, 不过后来它进行了修改. 这里是修改过后的流程.

> 输入: CART算法生成的决策树$T_{0}$
>
> 输出: 最优决策树$T_{\alpha}$
>
> 1. 设$k=1$, $T=T_0$, 最优子树集合$w = \{T_0\}$
> 2. 设$\alpha_{min}=+\infty$
> 3. **自下而上**的对所有的内部节点$t$计算$C(T_t)$(**分类树用基尼指数, 回归树用方差**), $|T_t|$, 以及$g(t) = \frac{C(t) - C(T_t)}{|T_t| - 1}$. 更新$\alpha_{min} = min(\alpha_{min}, g(t))$
> 4. 设$\alpha_{k} = \alpha_{min}$
> 5. **自上而下**的访问所有的内部节点$t$, 如果存在$\alpha_{k} \geq g(t)$, 即找到$\alpha$最小的节点进行剪枝, 并确定剪枝后的叶节点的类别. 由此, 我们获得了一颗新的树$T_{k}$, 将其加入到子树集合$w$
> 6. 参数更新: $T = T_k$, $k = k+1$
> 7. 如果$T_k$不是由根节点及两个叶节点构成的树, 则回到步骤2
> 8. 对子树集合$w$中的子树序列进行交叉验证, 从而获得最优的子树$T_{\alpha}$













