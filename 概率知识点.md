# 概率知识点

## 条件概率公式

$$
P(A|B) = \frac{P(AB)}{P(B)}
$$

其中$P(AB)$为联合概率, 在机器学习中也经常写成$P(X, Y)$.

由此我们可以把公式变形, 从而可以得到$P(AB) = P(B)P(A|B) = P(A)P(B|A)$.

将其进行推广我们可以得到如下公式, 直接把左式相乘即可得到.
$$
P(A_1, A_2,...,A_n) = P(A_1)P(A_2|A_1)...P(A_n|A_1A_2...A_{n-1})
$$

## 全概率公式

$$
P(B) = \sum_{i=1}^{n}P(a_i)P(B|A=a_i)
$$

这个公式可以直观进行理解, 即把$A$在各个取值情况下的$B$的概率计算出来, 那么求和就可以得到$B$的概率. 但这样的公式有一个前提, 即==$A$的各个取值是互斥的且能够构成整个样本空间, 即$A$对样本空间是完备的==. 

而如果$A$对样本空间不是完备的, 那么最终得到的结果就是联合概率.
$$
P(AB) = \sum_{i=1}^{n}P(a_i)P(B|A=a_i)
$$

## 贝叶斯公式

$$
P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{j=1}^{n}P(A|B_j)P(B_j)}
$$

对于贝叶斯公式的理解, 可以想成在已经得到$A$的结果的时候, 发生$B_i$的概率是多少. 即已知结果求是哪个条件引发结果的概率.