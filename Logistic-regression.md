# Logistic Regression

`Logistic Regression`是一个经典的分类算法, 它可以用于处理二分类以及多元分类问题. 虽然名字里面有"回归"两字, 但却不是一个回归算法.

## LR模型

常规的线性回归, 是通过参数和输入样本之间的运算从而得到输出结果($z=\theta^T x$). 但这个结果是连续的, 所以是回归模型. 而为了将其应用在分类问题当中, 我们需要对其应用转换函数, 即$g(z)$. 从而让其输出在某一区间内是类别A, 另一个区间内的是类别B, 由此得到一个分类模型.

为此我们选择`sigmoid`函数作为我们的激活函数, 具体的表达式为:
$$
g(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-\theta^T x}}
$$
它将输出转化成0-1区间内的值, 并且以0为界限输出分别大于0.5和小于0.5, 从而可以用来进行分类.

同时这个激活函数还有一个很好的**导数性质**, 需要我们记住:
$$
g'(z) = g(z)(1 - g(z))
$$

## 损失函数

这里介绍一下损失函数具体是如何推导过来的. 从激活函数的表达式中我们可以把输出分成两类(0和1), 从而有:
$$
\begin{aligned}
P(y=1; x, \theta) &= g_{\theta}(x) \\
P(y=0; x, \theta) &= 1- g_{\theta}(x)
\end{aligned}
$$
从而我们可以把两个等式写成一个式子:
$$
P(y; x, \theta) = g_{\theta}(x)^{y} (1-g_{\theta}(x))^{1-y}
$$
我们的目标是希望得到最优的参数$\theta$, 而在机器学习当中, 最常见的方法就是极大似然估计. 从而我们构建似然函数:
$$
L(\theta) = \prod_{i=1}^{n} g_{\theta}(x_i)^{y_i} (1-g_{\theta}(x_i)^{1-y_i})
$$
由于存在指数的形式, 所以可以使用对数进行处理, 从而得到:
$$
J(\theta) = -\frac{1}{n}lnL(\theta) = -\frac{1}{n}\sum_{i=1}^{n} y_ilog(g_{\theta}(x_i)) + (1-y_i)log(1-g_{\theta}(x_i))
$$
这样我们就获得了我们的损失函数表达式, 需要把整个过程融会贯通.

## 参数优化过程

对于损失函数极小化, 最常用的方法就是使用梯度下降法, 这里也是主要对这种方法进行详细的推导.

首先我们需要计算损失函数相对于参数的偏导数:
$$
\begin{aligned}
\frac{\part J(\theta) }{\part \theta} &= \frac{\part J(\theta)}{\part z} \frac{\part z}{\part \theta} \\
& = -\frac{1}{n}\sum_{i=1}^{n} [\frac{y_i}{g(z)}g'(z) + \frac{1-y_i}{1-g(z)}(-g'(z))]x_i \\
& = \frac{1}{n}\sum_{i=1}^{n} (g_{\theta}(x_i) - y_i)x_i
\end{aligned}
$$
从而我们可以使用梯度下降算法进行参数更新:
$$
\theta = \theta - \alpha \sum_{i=1}^{n}(g_{\theta}(x_i) - y_i)x_i
$$
其中$\alpha$是优化的步长.

对于一阶的优化方法, 除了上面提到的梯度下降, 可以再次基础上延伸出**随机梯度下降**, **mini随机梯度下降**.

而对于二阶的优化方法, 可以使用**牛顿法**, **拟牛顿法**.

## 正则化

介绍正则化之前, 我们需要先了解几个范数:

- `L0范数`: 向量中非0元素的个数

- `L1范数`: $||\theta||_{1}$, 即向量中所有元素绝对值之和

- `L2正则化`: $||\theta||_2^2$, 即向量的模

正则化的目的是解决过拟合, 主要体现在降低模型的复杂性, 即减小参数的数量或数值.

### 参数稀疏

为了减小参数的数量, 即增大参数的稀疏程度, 其实`L0范数`就可以实现, 但是实际的应用当中我们使用的都是`L1范数`. 这是因为`L0范数`是**不可微的**, 从而很难进行求解. 而`L1范数`可以看做是`L0范数`的近似, 且更加容易求解, 所以实际中通常使用的都是`L1范数`.

由于让参数稀疏, 即使部分的权值为0, 我们也可以用这种方法来进行参数的筛选.

### 参数数值降低

`L2范数`无法实现参数的稀疏, 但是它可以让参数的数值都尽量的小, 从而减小了模型的学习成本, 达到防止过拟合的目的.

## 多元LR

多元逻辑回归的主要思路就是将多元的问题转换成二元的问题. 其中主要有两种方法:

- `OvR`(one-vs-rest): 取其中一类为正类, 其他均为反类, 从而转换成二元问题进行识别.
- `MvM`(many-vs-many): 取其中一部分类别作为正类, 其他均为反类, 从而转换成二元问题进行识别.

## 优缺点

