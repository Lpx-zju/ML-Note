# 关于机器学习的面经

## 各种熵的计算

其中关于`信息熵`, `联合熵`, `条件熵`可以查看决策树的部分. 这里主要对`相对熵`和`交叉熵`进行介绍.

### 相对熵(KL散度)

相对熵主要用于衡量两个分布之间的相似程度, 相对熵的值越小, 则表明两个分布越接近.
$$
D_{KL}(p||q) = \sum_{i=1}^{n} p(x_i) log(\frac{p(x_i)}{q(x_i)})
$$
其中$p$表示原始的分布, 而$q$表示要评估的分布.

### 交叉熵

交叉熵在损失函数中经常遇见, 主要用于衡量目标值和预测值之间的差异程度. 而从其引申出来的交叉熵损失函数经常应用于分类任务当中.
$$
H(p, q) = -\sum_{i=1}^{n} p(x_i) log(q(x_i))
$$
其中$p$表示原始的分布, 而$q$表示要模型预测的分布.

从而我们可以很容易的得到, ==相对熵 = 交叉熵 - 信息熵==. 

在实际的训练当中, 我们实际需要的是`相对熵`, 即评估两个模型的相似程度, 我们希望预测的分布和真实的分布越接近越好. 但我们实际上不知道真实的分布, 所以使用训练集的分布来代替. 这样我们也就获取到了对应的信息熵, 从而求相对熵等价于求交叉熵, 因此使用交叉熵可以**用来评估模型训练的结果**.

## Bagging and Boosting

### RF和GBDT之前的区别

- RF的基学习器既可以是分类树也可以是回归树, **GBDT只能是回归树**
- RF可以**并行**生成基学习器, GBDT只能串行
- RF的结果是多票表决得到的, GBDT是多棵树累加得到的
- **RF对异常值不敏感, GBDT对异常值比较敏感**
- RF通过减少方差来提高性能, GBDT通过减少偏差来提高性能
- RF不需要进行数据预处理和归一化, 但GBDT需要, 因为可以帮助它收敛的更快

### GBDT和XGBoost之间的区别

- XGBoost不仅支持决策树, 还支持**线性分类器**
- 关于树的节点分裂方式, GBDT使用的是gini指数, 而XGBoost是经过优化推导后的
- GBDT只用到一阶导数, 而XGBoost用到了二阶
- XGBoost在代价函数里面应用了正则化
- XGBoost能够在特征选取和分裂点选取上实现并行