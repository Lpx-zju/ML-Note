# 关于机器学习的面经

## 各种熵的计算

其中关于`信息熵`, `联合熵`, `条件熵`可以查看决策树的部分. 这里主要对`相对熵`和`交叉熵`进行介绍.

### 相对熵(KL散度)

相对熵主要用于衡量两个分布之间的相似程度, 相对熵的值越小, 则表明两个分布越接近.
$$
D_{KL}(p||q) = \sum_{i=1}^{n} p(x_i) log(\frac{p(x_i)}{q(x_i)})
$$
其中$p$表示原始的分布, 而$q$表示要评估的分布.

### 交叉熵

交叉熵在损失函数中经常遇见, 主要用于衡量目标值和预测值之间的差异程度. 而从其引申出来的交叉熵损失函数经常应用于分类任务当中.
$$
H(p, q) = -\sum_{i=1}^{n} p(x_i) log(q(x_i))
$$
其中$p$表示原始的分布, 而$q$表示要模型预测的分布.

从而我们可以很容易的得到, ==相对熵 = 交叉熵 - 信息熵==. 

在实际的训练当中, 我们实际需要的是`相对熵`, 即评估两个模型的相似程度, 我们希望预测的分布和真实的分布越接近越好. 但我们实际上不知道真实的分布, 所以使用训练集的分布来代替. 这样我们也就获取到了对应的信息熵, 从而求相对熵等价于求交叉熵, 因此使用交叉熵可以**用来评估模型训练的结果**.