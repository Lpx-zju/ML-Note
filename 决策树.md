# 决策树

决策树算法即可以作为**分类算法**, 也可以作为**回归算法**, 同时也特别适合**集成学习**. 这里面需要了解的是`ID3`, `C4.5`, `CART`三类算法. 并且掌握它的算法实现.

决策树的核心需要解决的方法就是, 在走到一个节点之后, 应该如何**选择合适的特征**作为分叉的标准.

## ID3

### 信息熵

信息熵可以说是ID3算法的核心. 熵代表着事物的不确定性, **不确定性越高, 则熵越大**. 信息熵的表达式如下所示.
$$
H(X) = -\sum_{x \in X}P(X = x)logP(X = x)
$$
其中$n$代表$X$的不同取值, 而$p_i$则代表$X$取值为$i$时的概率, $log$是以2为底或者是以e为底的对数. 

例如当$X$为2时, 此时只有两个概率取值均为1/2时, 信息熵的值才会最大, 即此时的不确定性最大.

#### 联合信息熵

此外, 我们也可以推出多个变量的联合熵的表达式.
$$
H(X, Y) = -\sum_{x \in X}\sum_{y \in Y}P(X=x, Y=y)logP(X=x, Y=y)
$$
联合熵表达式还有其他的一些写法, 但我认为这样的写法最容易理解, 即遍历$X$和$Y$的取值, 求所有功能的信息熵再求和.

#### 条件熵

条件熵的物理意义是在得知某一确定信息的前提下, 另一条信息的不确定性. 我们可以得到条件熵的公式及其推导过程如下所示.
$$
\begin{aligned}
H(Y|X) &= \sum_{x \in X}P(X=x)H(Y|X=x) \\
       &= -\sum_{x \in X}P(X=x)\sum_{y \in Y}P(y|X=x)logP(y|X=x) \\
       &= -\sum_{x \in X}\sum_{y \in Y}P(X=x)P(y|X=x)logP(y|X=x) \\
       &= -\sum_{x \in X}\sum_{y \in Y}P(X=x, Y=y)logP(y|X=x)
\end{aligned}
$$
==需要注意, 这个条件熵和条件概率是不同的.==条件概率是指在变量$X$取某一个值时, 另一个变量发生的概率是多少. 而在推导条件熵的过程中, 会用到类似的概念, 我把这个值称为**类条件熵**, 其具体的公式如下所示.
$$
H(Y|X=x) = \sum_{y \in Y}P(y|X=x)logP(y|X=x)
$$
从而我们可以发现, 条件熵其实是类条件熵的一个**期望**, 意思是在一个变量$X$的条件下(==变量$X$的每个值都会取到==), 另一个变量$Y$熵对X变量的期望. $\color{#0000FF}{不要把条件熵的概念和类条件熵混淆了!}$

### 信息增益

$$
I(X, Y) = H(X) - H(X|Y)
$$

这个物理意义是比较好理解的, 即体现了在知道信息的Y的条件下, X的不确定性下降的程度.

### 算法流程

ID3中使用`信息增益`作为判断依据, 来选择特征构建决策树. 具体的算法过程如下.

```markdown
1. 初始化信息增益阈值
2. 判断样本是否为同一类输出, 如果是则返回单节点树, 并标记类别
3. 判断特征是否为空, 如果是则返回单节点树, 标记类别为类别样本数最多的类别
4. 计算各个特征对输出的信息增益, 选择信息增益最大的特征
5. 如果该特征的信息增益小于阈值, 怎返回单节点树, 标记类别为类别样本数量最多的类别
6. 否则, 按照特征的取值分成不同的分支, 返回增加的节点数
7. 对于所有的子节点, 递归的调用2-6步, 从而得到子树返回.
```

### 不足

- 没有考虑连续特征, 如密度等, 限制了应用范围
- 由于采用了信息增益, 导致被发现特征取值比较多的情况下其信息增益大. 即**更倾向于取值多的特征**
- 没有考虑缺失值的情况
- 没有应对过拟合的措施

## C4.5

C4.5算法主要是针对于ID3中的不足提出了一些改进方法.

### 针对不适用连续特征问题

采用的方法是将连续特征进行离散化. 例如, 对于$m$个样本, 选择$m-1$个划分点, 对于每一个划分点计算信息增益, 从而选择最优的划分点将样本划分成两类, 从而实现连续特征的离散化.

### 针对更偏向于取值多的特征的问题

引入了信息增益比,  表达式如下所示.
$$
I_R(D, A) = \frac{I(D, A)}{H_A(D)}
$$
信息增益比是在原来的基础上除以$H_A(D)$得到的, 我们称它为信息熵, 即具体的表达式如下.
$$
H_A(D) = -\sum_{i=1}^{n}\frac{|D_i|}{|D|}log\frac{|D_i|}{|D|}
$$
其中$n$为特征$A$的类别数, $|D|$为样本个数, $|D_i|$为特征取第i个值得样本数. 这个值具有**取值种类越多, 则值越大**的特点. 因此将其作为分母, 可以有效的修正偏向取值多的特征的问题.

> 对于后面两个问题,  在CART中都有更好的解法, 故不在此赘述了.

### 不足

- 虽然针对过拟合使用了剪枝, 但还可以进一步的优化
- C4.5生成的是多叉树的模型, 而在计算机中, 二叉树的模型往往效率更高一些
- **C4.5只能用于分类**
- 由于使用了熵模型, 并且对连续变量进行了大量的排序运算, 所以运算强度大, 耗时严重.

## CART

无论是在ID3还是在C4.5当中, 均是使用熵模型来计算不确定度, 这带来了极大的计算压力. 而CART为了避免这个问题, 使用了**基尼系数**来替代信息增益比.

### 基尼系数

基尼系数代表了信息的不纯度, 即==基尼系数越小, 则纯度越高, 特征越好.==

具体的, 假设有$K$个类别, 其中第$k$个类别的概率是$P_k$, 则基尼系数的表达式如下所示.
$$
Gini(P) = \sum_{k=1}^{K}P_k(1-P_k) = 1 - \sum_{k=1}^{K}P_k^2
$$
而在实际的使用过程中, 各个类别的概率我们是不知道的, 从而使用样本进行估计, 得到如下的表达式. 其中$|C_k|$为取值为$k$的样本量.
$$
Gini(D) = 1 - \sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$
而如果根据特征$A$将样本划分成两个部分, 则在特征$A$的条件下, 其基尼系数的表达式如下所示.
$$
Gini(D, A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
$$
CART树通过使用基尼系数, 从而大大提高了运算效率. 同时为了进一步的简化, **CART每次仅仅对某一个特征的值进行二分**, 因此CART树是一棵``二叉树``. 这样可以进一步的简化运算.

### 对于连续值和离散值的处理

对于连续值的处理和C4.5的方法是一样的, 都是通过排序, 再选择相邻两值的均值作为一个划分点, 从而选择最优的划分点. 只不过C4.5中使用的是信息增益比, 而CART中使用的是基尼系数.

而对于离散值, 由于CART是一棵二叉树, 因此它采用的是不断二分. 如对于特征$A$要建立决策树节点, 它有$A_1, A_2, A_3$三个类别. 如果是在C4.5中, 会直接分出三个分支, 即不同的类别为不同的分支. ==但是在CART中, 只会分成两个分支, 如{$A_1$}{$A_2, A_3$}等.==并从中选择基尼系数最小的分割方法进行分割, 从而仅生成两个分支. 这是两者最大的区别.

### 算法流程

这里仅仅介绍的建立的算法流程, CART中还会涉及到剪枝的算法.

```markdown
输入: 训练集D, 基尼系数的阈值, 样本个体阈值
输出: 决策树T

1. 对于当前节点的数据集D, 如果其数量小于样本个体阈值, 则返回决策子树, 递归停止
2. 计算当前数据集D的基尼指数, 如果基尼指数小于阈值, 则返回决策子树, 递归停止
3. 计算当前各个特征和特征值的基尼指数, 对于离散值和连续值采用不同的策略
4. 选择基尼系数最小的特征A和它对应的特征值a, 根据这个最优特征和最优特征值, 将数据集进行二分
5. 对左右子树继续递归调用1-4步, 指导递归结束, 返回生成的决策树T
```

### CART回归树

回归树是CART树的一大特点, 因为ID3和C4.5都只能用于进行分类, 但CART树既可以用作分类, 也可以用作回归. 而分类树和回归树的区别主要在于输出值. 如果输出是离散的, 那么就是分类树. 如果输出是连续的, 那么就是回归树.

CART回归树在算法上和分类树基本都是相同的, 这里只是提一下不同点.

- 连续值的处理方式不同

  在回归树中, 对于连续值的处理, 使用的是`方差`的度量方式. 也正是这种度量方式的选择, 才能让CART具有回归的功能. 因为无论是熵模型还是基尼系数, 都只适用于分类.

  CART回归树的度量目标是, 对于任意划分特征$A$, 对应的任意划分点$s$两边分成的数据集分别是$D_1$和$D_2$. **求出使各自集合的均方差最小, 同时均方差之和最小所对应的最优特征和最优划分点. 表达式如下所示. 其中$c_1$为数据集$D_1$的均值, $c_2$是数据集$D_2$的均值.

$$
\underbrace{min}_{A, s}[\underbrace{min}_{c_1}\sum_{x_i \in D_1(A, s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum_{x_i \in D_2(A, s)}(y_i - c_2)^2]
$$

- 预测方式的不同

  对于分类问题, CART输出的是节点中的样本量最多的类别. 而对于回归问题, CART输出的是**节点中的均值或者中位数**.

### CART中的剪枝

CART分类树和CART剪枝树核心的不同为度量损失的不同, 因此在剪枝的算法基本是一样的.