# 决策树

决策树算法即可以作为**分类算法**, 也可以作为**回归算法**, 同时也特别适合**集成学习**. 这里面需要了解的是`ID3`, `C4.5`, `CART`三类算法. 并且掌握它的算法实现.

决策树的核心需要解决的方法就是, 在走到一个节点之后, 应该如何**选择合适的特征**作为分叉的标准.

## ID3

### 信息熵

信息熵可以说是ID3算法的核心. 熵代表着事物的不确定性, **不确定性越高, 则熵越大**. 信息熵的表达式如下所示.
$$
H(X) = -\sum_{x \in X}P(X = x)logP(X = x)
$$
其中$n$代表$X$的不同取值, 而$p_i$则代表$X$取值为$i$时的概率, $log$是以2为底或者是以e为底的对数. 

例如当$X$为2时, 此时只有两个概率取值均为1/2时, 信息熵的值才会最大, 即此时的不确定性最大.

#### 联合信息熵

此外, 我们也可以推出多个变量的联合熵的表达式.
$$
H(X, Y) = -\sum_{x \in X}\sum_{y \in Y}P(X=x, Y=y)logP(X=x, Y=y)
$$
联合熵表达式还有其他的一些写法, 但我认为这样的写法最容易理解, 即遍历$X$和$Y$的取值, 求所有功能的信息熵再求和.

#### 条件熵

条件熵的物理意义是在得知某一确定信息的前提下, 另一条信息的不确定性. 我们可以得到条件熵的公式及其推导过程如下所示.
$$
\begin{aligned}
H(Y|X) &= \sum_{x \in X}P(X=x)H(Y|X=x) \\
       &= -\sum_{x \in X}P(X=x)\sum_{y \in Y}P(y|X=x)logP(y|X=x) \\
       &= -\sum_{x \in X}\sum_{y \in Y}P(X=x)P(y|X=x)logP(y|X=x) \\
       &= -\sum_{x \in X}\sum_{y \in Y}P(X=x, Y=y)logP(y|X=x)
\end{aligned}
$$
==需要注意, 这个条件熵和条件概率是不同的.==条件概率是指在变量$X$取某一个值时, 另一个变量发生的概率是多少. 而在推导条件熵的过程中, 会用到类似的概念, 我把这个值称为**类条件熵**, 其具体的公式如下所示.
$$
H(Y|X=x) = \sum_{y \in Y}P(y|X=x)logP(y|X=x)
$$
从而我们可以发现, 条件熵其实是类条件熵的一个**期望**, 意思是在一个变量$X$的条件下(==变量$X$的每个值都会取到==), 另一个变量$Y$熵对X变量的期望. $\color{#0000FF}{不要把条件熵的概念和类条件熵混淆了!}$

### 信息增益

$$
I(X, Y) = H(X) - H(X|Y)
$$

这个物理意义是比较好理解的, 即体现了在知道信息的Y的条件下, X的不确定性下降的程度.

### 算法思路

ID3中使用`信息增益`作为判断依据, 来选择特征构建决策树. 具体的算法过程如下.

```
1. 初始化信息增益阈值
2. 判断样本是否为同一类输出, 如果是则返回单节点树, 并标记类别
3. 判断特征是否为空, 如果是则返回单节点树, 标记类别为类别样本数最多的类别
4. 计算各个特征对输出的信息增益, 选择信息增益最大的特征
5. 如果该特征的信息增益小于阈值, 怎返回单节点树, 标记类别为类别样本数量最多的类别
6. 否则, 按照特征的取值分成不同的分支, 返回增加的节点数
7. 对于所有的子节点, 递归的调用2-6步, 从而得到子树返回.
```

### 不足

- 没有考虑连续特征, 如密度等, 限制了应用范围
- 由于采用了信息增益, 导致被发现特征取值比较多的情况下其信息增益大. 即**更倾向于取值多的特征**
- 没有考虑缺失值的情况
- 没有应对过拟合的措施

## C4.5

C4.5算法主要是针对于ID3中的不足提出了一些改进方法.

### 针对不适用连续特征问题

采用的方法是将连续特征进行离散化. 例如, 对于$m$个样本, 选择$m-1$个划分点, 对于每一个划分点计算信息增益, 从而选择最优的划分点将样本划分成两类, 从而实现连续特征的离散化.

### 针对更偏向于取值多的特征的问题

引入了信息增益比,  表达式如下所示.
$$
I_R(D, A) = \frac{I(D, A)}{H_A(D)}
$$
信息增益比是在原来的基础上除以$H_A(D)$得到的, 我们称它为信息熵, 即具体的表达式如下.
$$
H_A(D) = -\sum_{i=1}^{n}\frac{|D_i|}{|D|}log\frac{|D_i|}{|D|}
$$
其中$n$为特征$A$的类别数, $|D|$为样本个数, $|D_i|$为特征取第i个值得样本数. 这个值具有**取值种类越多, 则值越大**的特点. 因此将其作为分母, 可以有效的修正偏向取值多的特征的问题.

> 对于后面两个问题,  在CART中都有更好的解法, 故不在此赘述了.

### 不足

- 虽然针对过拟合使用了剪枝, 但还可以进一步的优化
- C4.5生成的是多叉树的模型, 而在计算机中, 二叉树的模型往往效率更高一些
- **C4.5只能用于分类**
- 由于使用了熵模型, 并且对连续变量进行了大量的排序运算, 所以运算强度大, 耗时严重.

## CART

无论是在ID3还是在C4.5当中, 均是使用熵模型来计算不确定度, 这带来了极大的计算压力. 而CART为了避免这个问题, 使用了**基尼系数**来替代信息增益比.

### 基尼系数

基尼系数代表了信息的不纯度, 即==基尼系数越小, 则纯度越高, 特征越好.==

具体的, 假设有$K$个类别, 其中第$k$个类别的概率是$P_k$, 则基尼系数的表达式如下所示.
$$
Gini(P) = \sum_{k=1}^{K}P_k(1-P_k) = 1 - \sum_{k=1}^{K}P_k^2
$$
而在实际的使用过程中, 各个类别的概率我们是不知道的, 从而使用样本进行估计, 得到如下的表达式. 其中$|C_k|$为取值为$k$的样本量.
$$
Gini(D) = 1 - \sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$
而如果根据特征$A$将样本划分成两个部分, 则在特征$A$的条件下, 其基尼系数的表达式如下所示.
$$
Gini(D, A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
$$
CART树通过使用基尼系数, 从而大大提高了运算效率. 同时为了进一步的简化, **CART每次仅仅对某一个特征的值进行二分**, 因此CART树是一棵``二叉树``. 这样可以进一步的简化运算.

